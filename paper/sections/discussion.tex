\section{Discussion}

\begin{comment}
    Here you discuss how your results are: (1) in-line with previous work; and (2) differ
    (expand) on previous work. Also, you can list the limitations of your work (link to future work)

    (a) Which results match previous findings in the literature?
    (b) Which results differ from previous findings, and why
\end{comment}

Our evaluation shows that exposing decision-makers to simulated deliberations from diverse organizational roles significantly improves the quality of ethical reasoning across multiple dimensions. Participants in the treatment group produced responses that were clearer, more relevant, more persuasive, and more contextually and ethically aware. We presented results that were consistent, robust, and supported by both qualitative self-assessments and quantitative metrics.

These findings align with long-standing claims in the organizational ethics literature that decision-making quality improves when diverse perspectives are considered prior to action \cite{donaldson_preston_1995, mitchell2020stakeholder}. However, prior work has largely theorized this benefit without offering scalable, practical mechanisms to operationalize it. Our results provide empirical evidence that simulating internal plurality, even through artificial means, can support this goal.

In contrast to much of the existing work in computer science ethics, which focuses on algorithmic fairness and AI alignment at the system-output level \cite{mehrabi2022survey, gabrielartificial}, our work expands the scope of technical intervention to include the human processes around AI, specifically the moral deliberation structures within organizations. Recent advances in agent-based modeling and LLM-driven social simulation \cite{park_etal_2024_1000, gilbert_2022} suggest the potential of AI for mirroring social reasoning, but these approaches have not yet been applied to support human decision-making in situ. To our knowledge, this is the first empirical evaluation of a plural-agent tool in a high-stakes professional context using senior decision-makers.

Our study also highlights the potential of modular, interpretable AI outputs in applied ethics tools. Unlike end-to-end decision models, our summaries serve as intermediate reasoning aids, allowing users to maintain agency while still benefiting from computational support.
