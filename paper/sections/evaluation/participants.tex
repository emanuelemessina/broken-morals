\subsection{Participants Recruitment}

To evaluate the effectiveness of our tool with a relevant population, we recruited participants through the Prolific platform, targeting professionals in the technology sector with decision-making responsibilities in high-stakes organizational contexts.
We applied several screening criteria to ensure both relevance and quality of the responses. Eligible participants respected the following criteria:
(1) currently residing in an english speaking
(2) holding a senior work role
(3) having a 100\% approval rate on previous Prolific submissions
The exact screeners we used can be found in Description \ref{desc:screeners}.
These screeners were designed to increase the likelihood of finding fluent English speakers to reduce language-related confounds, and ensure that participants would possess both the soft skills and practical experience necessary to produce grounded, high-quality justifications when responding to complex moral dilemmas.
The compensation was set at Â£9 per hour, aligning with Prolific's recommended fair pay rate for tasks of this type.
Participants were shown a short study description prior to consenting, emphasizing that the task involved reading a scenario and providing a written answer, and asking them explicitly not to use AI tools in order to preserve the integrity of the human reasoning evaluation. The full participant-facing prompt is available in Description \ref{desc:study}.

\subsection{Participant Groups and Tasks}
control, treatment, form format, pay, time, task, questionnaires
