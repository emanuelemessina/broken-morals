\subsubsection{Participants Recruitment}

To evaluate the effectiveness of our tool with a relevant population, we recruited 50 participants through the Prolific platform, targeting professionals in the technology sector with decision-making responsibilities in high-stakes organizational contexts.
We applied several screening criteria to ensure both relevance and quality of the responses. Eligible participants respected the following criteria:
(1) currently residing in an english speaking country
(2) holding a senior work role
(3) having a 100\% approval rate on previous Prolific submissions
The exact screeners we used can be found in Description \ref{desc:screeners}.
These screeners were designed to increase the likelihood of finding fluent English speakers to reduce language-related confounds, and ensure that participants would possess both the soft skills and practical experience necessary to produce grounded, high-quality justifications when responding to complex moral dilemmas.
The compensation was set at Â£9 per hour, aligning with Prolific's recommended fair pay rate for tasks of this type.
Participants were shown a short study description prior to consenting, emphasizing that the task involved reading a scenario and providing a written answer, and asking them explicitly not to use AI tools in order to preserve the integrity of the human reasoning evaluation. The full participant-facing prompt is available in Description \ref{desc:study}.

\subsubsection{Participant Groups and Tasks}

We randomly assigned the 50 participants into two groups of 25 participants each: the Control and the Treatment groups.
We further divided each group into five subgroups, one for each dilemma we selected (Section \ref{sec:dilemmas}).
Each of the selected dilemmas was assigned to one subgroup in both the Control and Treatment groups, such that any participant received only one dilemma in their survey, and all the selected dilemmas were covered.
In this way we ensured each dilemma was answered by exactly the same number of people in each group, and we reduced the participants' mental fatigue when completing the survey.
The structure of the task differed slightly between the two groups.
In the Control group, participants read the dilemma and directly answered its question(s).
In the Treatment group, participants read the same dilemma and question(s), followed by the tool-generated summary (Section \ref{sec:summary}).
The full list of summaries shown to the Treatment group for each dilemma is reported in Appendix \ref{sec:selected_dilemmas}.
After completing their answers, all participants filled out a short post-task questionnaire to assess their effort and percieved difficulty of the dilemma.
For the treatment group, this also included questions on perceived usefulness of the tool output, and whether it influenced their thinking.
All questions in the final questionnaire where formulated on a Likert scale from 1 to 5.
Exact questionnaire wording and task templates for both groups are available in Appendix \ref{sec:form}.
To ensure response quality, all participants encountered a simple attention-check questions midway through the task, designed to verify that the dilemma had been read carefully. Responses that failed this check were excluded from analysis.
